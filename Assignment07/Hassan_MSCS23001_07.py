# -*- coding: utf-8 -*-
"""hw_07_DL_submit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mmmgAo015d2PNuoPpxybfxWjVh7_xQNS
"""

"""
###<u> **DEEP LEARNING PROGRAMMING ASSIGNMENT # 7** </u>
* **NAME = HASSAN JAVAID**
* **ROLL NO. = MSCS23001**
* **TASK01 = Implementation of Diffusion Model**
* **ALGORITHM used: Diffusion Model**
"""


import re
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, datasets
from torchsummary import summary
import matplotlib.colors as mcolors
import torchvision.utils as vutils
from tqdm.auto import tqdm
from collections import Counter
import logging
import torchvision

from google.colab import drive
drive.mount('/content/drive')

"""# Dataset & Function Definitions"""

class AnimalDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):

        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]
        img_name = os.path.basename(img_path)

        if self.transform:
            image = self.transform(image)

        return image, label, img_name


def create_dataset(root_dir, transform=None):
    image_paths = []
    labels = []
    label_names = os.listdir(root_dir)

    # Create a list of image file paths and their corresponding labels
    for label_idx, label_name in enumerate(label_names):
        class_folder = os.path.join(root_dir, label_name)
        for img_name in os.listdir(class_folder):
            if img_name.lower().endswith(('png', 'jpg', 'jpeg')):
                image_paths.append(os.path.join(class_folder, img_name))
                labels.append(label_idx)

    return AnimalDataset(image_paths=image_paths, labels=labels, transform=transform)


def plotLoss(epoch_losses):
    plt.figure(figsize=(10, 5))
    plt.plot(epoch_losses, label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Over Epochs')
    plt.legend()
    plt.show()


def print_dataset_stats(dataset, label_names):
    total_images = len(dataset)
    class_counts = Counter(dataset.labels)

    print(f"Total number of images: {total_images}")
    for label, count in class_counts.items():
        print(f"Number of images in class '{label_names[label]}': {count}")

"""#Create Dataset"""

root_dir = '/content/drive/MyDrive/Assignment07/animal_data'

IMAGE_SIZE = 64

transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
])

animal_dataset = create_dataset(root_dir=root_dir, transform=transform)


label_names = os.listdir(root_dir)

# Print dataset statistics
print_dataset_stats(animal_dataset, label_names)

"""#Split the datasets"""

# Implements function to split a dataset into train, validation, and test sets
def split_dataset(dataset, train_ratio=0.75, valid_ratio=0.1, test_ratio=0.15, random_state=None):
    total_size = len(dataset)
    indices = list(range(total_size))

    train_size = train_ratio
    valid_size = valid_ratio / (valid_ratio + test_ratio)

    train_indices, temp_indices = train_test_split(indices, train_size=train_size, random_state=random_state)
    valid_indices, test_indices = train_test_split(temp_indices, train_size=valid_size, random_state=random_state)

    train_dataset = Subset(dataset, train_indices)
    valid_dataset = Subset(dataset, valid_indices)
    test_dataset = Subset(dataset, test_indices)

    return train_dataset, valid_dataset, test_dataset

# Split the dataset
train_dataset, valid_dataset, test_dataset = split_dataset(animal_dataset,
                                                           train_ratio=0.75,
                                                           valid_ratio=0.1,
                                                           test_ratio=0.15,
                                                           random_state=42)

# def print_dataset_stats_datsets(train_dataset, valid_dataset, test_dataset, label_names):
#     train_labels = [label for _, label, _ in train_dataset]
#     valid_labels = [label for _, label, _ in valid_dataset]
#     test_labels = [label for _, label, _ in test_dataset]

#     train_counts = Counter(train_labels)
#     valid_counts = Counter(valid_labels)
#     test_counts = Counter(test_labels)

#     print("Train Dataset:")
#     for label, count in train_counts.items():
#         print(f"Number of images in class '{label_names[label]}': {count}")

#     print("\nValidation Dataset:")
#     for label, count in valid_counts.items():
#         print(f"Number of images in class '{label_names[label]}': {count}")

#     print("\nTest Dataset:")
#     for label, count in test_counts.items():
#         print(f"Number of images in class '{label_names[label]}': {count}")

# # Print dataset statistics
# print_dataset_stats_datsets(train_dataset, valid_dataset, test_dataset, label_names)

"""#Save and load the datasets"""

def save_datasets(train_dataset, valid_dataset, test_dataset, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    torch.save(train_dataset, os.path.join(save_dir, f'train_dataset_{IMAGE_SIZE}_all.pt'))
    torch.save(valid_dataset, os.path.join(save_dir, f'valid_dataset_{IMAGE_SIZE}_all.pt'))
    torch.save(test_dataset, os.path.join(save_dir, f'test_dataset_{IMAGE_SIZE}_all.pt'))


def load_datasets(load_dir):
    train_dataset = torch.load(os.path.join(load_dir, f'train_dataset_{IMAGE_SIZE}_all.pt'))
    valid_dataset = torch.load(os.path.join(load_dir, f'valid_dataset_{IMAGE_SIZE}_all.pt'))
    test_dataset = torch.load(os.path.join(load_dir, f'test_dataset_{IMAGE_SIZE}_all.pt'))
    return train_dataset, valid_dataset, test_dataset


save_dir = root_dir + '/data/saved_datasets'

# # Save datasets
# save_datasets(train_dataset, valid_dataset, test_dataset, save_dir)

# Load datasets
loaded_train_dataset, loaded_valid_dataset, loaded_test_dataset = load_datasets(load_dir=save_dir)

"""#Making Dataloader"""

dataloader = DataLoader(loaded_train_dataset, batch_size=32, shuffle=True)

"""#Show Dataloader Images"""

def displayImagesSub(images, title=""):

    if isinstance(images, torch.Tensor):
        images = images.detach().cpu().numpy()

    fig = plt.figure(figsize=(12, 12))
    rows = int(len(images) ** (1 / 2))
    cols = round(len(images) / rows)

    idx = 0
    for r in range(rows):
        for c in range(cols):
            if idx < len(images):
                ax = fig.add_subplot(rows, cols, idx + 1)
                image = images[idx]

                # Normalize image if it's a float and out of range
                if image.dtype == np.float32 or image.dtype == np.float64:
                    image = np.clip(image, 0, 1)
                elif image.max() > 255:
                    image = image / 255.0

                if image.ndim == 2 or image.shape[0] == 1:  # Grayscale image
                    ax.imshow(image.squeeze(), cmap='gray')
                else:
                    # RGB image
                    ax.imshow(image.transpose(1, 2, 0))

                idx += 1

    fig.suptitle(title, fontsize=30)

    # Showing the figure
    plt.show()

def displayFirstBatch(loader):
    for batch in loader:
        displayImagesSub(batch[0], "Sample Images for 1st Batch")
        break

# Show a batch of regular images
displayFirstBatch(dataloader)

"""#Getting Device"""

# Getting device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}" if torch.cuda.is_available() else "CPU")

"""#Diffusion Model Class"""

# My Diffusion class
class MyDiffusion(nn.Module):
    def __init__(self, network, t_steps, min_beta, max_beta, device=None, image_chw=(3, 64, 64)):
        super(MyDiffusion, self).__init__()
        self.t_steps = t_steps
        self.min_beta = min_beta
        self.max_beta = max_beta
        self.device = device
        # self.image_chw = image_chw
        self.network = network.to(device)
        self.betas = torch.linspace(self.min_beta , self.max_beta, self.t_steps).to(device)
        self.alphas = 1 - self.betas
        self.alpha_bars = torch.cumprod(self.alphas, dim=0).to(device)

    def forward(self, x, t, eta=None):

        n, c, h, w = x.shape
        a_bar = self.alpha_bars[t]

        if eta is None:
            eta = torch.randn(n, c, h, w).to(self.device)

        noisy_imgs = a_bar.sqrt().reshape(n, 1, 1, 1) * x + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta
        return noisy_imgs

    def backward(self, x, t):
        # Run each image through the network for each timestep t in the vector t.
        # The network returns its estimation of the noise that was added.
        return self.network(x, t)

"""#Unet Architecture Cutomized with time embeddings"""

# Implements the standard positional embedding for noise calcualtion
def sinusoidal_embedding(n, d):
    embedding = torch.zeros(n, d)
    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])
    wk = wk.reshape((1, d))
    t = torch.arange(n).reshape((n, 1))
    embedding[:,::2] = torch.sin(t * wk[:,::2])
    embedding[:,1::2] = torch.cos(t * wk[:,::2])
    return embedding


# Implements fundamental block for U-Net Block
class MyBlock(nn.Module):
    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):
        super(MyBlock, self).__init__()
        self.ln = nn.LayerNorm(shape)
        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)
        self.activation = nn.SiLU() if activation is None else activation
        self.normalize = normalize

    def forward(self, x):
        out = self.ln(x) if self.normalize else x
        out = self.conv1(out)
        out = self.activation(out)
        out = self.conv2(out)
        out = self.activation(out)
        return out


# Implements customized class U-Net for image dimension (3 x 64 x 64)
class MyUNet(nn.Module):
    def __init__(self, t_steps=1000, time_emb_dim=100):
        super(MyUNet, self).__init__()

        # Sinusoidal embedding
        self.time_embed = nn.Embedding(t_steps, time_emb_dim)
        self.time_embed.weight.data = sinusoidal_embedding(t_steps, time_emb_dim)
        self.time_embed.requires_grad_(False)

        # Encoder Part
        self.te1 = self.buildTimeEmbedding(time_emb_dim, 3)
        self.b1 = nn.Sequential(
            MyBlock((3, 64, 64), 3, 64),
            MyBlock((64, 64, 64), 64, 64),
            MyBlock((64, 64, 64), 64, 64)
        )
        self.down1 = nn.Conv2d(64, 64, 4, 2, 1)

        self.te2 = self.buildTimeEmbedding(time_emb_dim, 64)
        self.b2 = nn.Sequential(
            MyBlock((64, 32, 32), 64, 128),
            MyBlock((128, 32, 32), 128, 128),
            MyBlock((128, 32, 32), 128, 128)
        )
        self.down2 = nn.Conv2d(128, 128, 4, 2, 1)

        self.te3 = self.buildTimeEmbedding(time_emb_dim, 128)
        self.b3 = nn.Sequential(
            MyBlock((128, 16, 16), 128, 256),
            MyBlock((256, 16, 16), 256, 256),
            MyBlock((256, 16, 16), 256, 256)
        )
        self.down3 = nn.Sequential(
            nn.Conv2d(256, 256, 3, 1, 1),
            nn.SiLU(),
            nn.Conv2d(256, 256, 4, 2, 1)
        )

        # Bottleneck
        self.te_mid = self.buildTimeEmbedding(time_emb_dim, 256)
        self.b_mid = nn.Sequential(
            MyBlock((256, 8, 8), 256, 128),
            MyBlock((128, 8, 8), 128, 128),
            MyBlock((128, 8, 8), 128, 256)
        )

        # Decoder Part
        self.up1 = nn.Sequential(
            nn.ConvTranspose2d(256, 256, 4, 2, 1),
            nn.SiLU(),
            nn.Conv2d(256, 256, 3, 1, 1)
        )

        self.te4 = self.buildTimeEmbedding(time_emb_dim, 512)
        self.b4 = nn.Sequential(
            MyBlock((512, 16, 16), 512, 256),
            MyBlock((256, 16, 16), 256, 128),
            MyBlock((128, 16, 16), 128, 128)
        )

        self.up2 = nn.ConvTranspose2d(128, 128, 4, 2, 1)
        self.te5 = self.buildTimeEmbedding(time_emb_dim, 256)
        self.b5 = nn.Sequential(
            MyBlock((256, 32, 32), 256, 128),
            MyBlock((128, 32, 32), 128, 64),
            MyBlock((64, 32, 32), 64, 64)
        )

        self.up3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)
        self.te_out = self.buildTimeEmbedding(time_emb_dim, 128)
        self.b_out = nn.Sequential(
            MyBlock((128, 64, 64), 128, 64),
            MyBlock((64, 64, 64), 64, 64),
            MyBlock((64, 64, 64), 64, 64, normalize=False)
        )

        self.conv_out = nn.Conv2d(64, 3, 3, 1, 1) # Converts to 3 out channels

    def buildTimeEmbedding(self, dim_in, dim_out):
        return nn.Sequential(
            nn.Linear(dim_in, dim_out),
            nn.SiLU(),
            nn.Linear(dim_out, dim_out)
        )


    def forward(self, x, t):
        # (image with positional embedding stacked on channel dimension)
        # x is (N, 3, 64, 64)

        t = self.time_embed(t)
        n = len(x)

        enc_out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))     # (N, 64, 64, 64)
        enc_out2 = self.b2(self.down1(enc_out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 128, 32, 32)
        enc_out3 = self.b3(self.down2(enc_out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 256, 16, 16)

        out_mid = self.b_mid(self.down3(enc_out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 256, 8, 8)

        dec_out4 = torch.cat((enc_out3, self.up1(out_mid)), dim=1)        # (N, 512, 16, 16)
        dec_out4 = self.b4(dec_out4 + self.te4(t).reshape(n, -1, 1, 1))   # (N, 128, 16, 16)

        dec_out5 = torch.cat((enc_out2, self.up2(dec_out4)), dim=1)       # (N, 256, 32, 32)
        dec_out5 = self.b5(dec_out5 + self.te5(t).reshape(n, -1, 1, 1))   # (N, 128, 32, 32)

        out = torch.cat((enc_out1, self.up3(dec_out5)), dim=1)        # (N, 128, 64, 64)
        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))   # (N, 3, 64, 64)

        out = self.conv_out(out)

        return out

"""#Instantiating the model"""

# Instantiate the model
t_steps, min_beta, max_beta = 500, 10 ** -4, 0.02
model = MyDiffusion(MyUNet(t_steps), t_steps=t_steps, min_beta=min_beta, max_beta=max_beta, device=device)

sum([p.numel() for p in model.parameters()])

"""#Custom MSE Loss"""

class CustomMSELoss(nn.Module):
    def __init__(self):
        super(CustomMSELoss, self).__init__()

    def forward(self, input, target):
        return torch.mean((input - target) ** 2)


"""#Forward Diffusion Process Visualizations"""

def displayForward(model, loader, device):
    # Showing the forward process
    for batch in loader:
        imgs = batch[0]

        displayImagesSub(imgs, "Original images")

        for percent in [0.25, 0.5, 0.75, 1]:
            displayImagesSub(
                model(imgs.to(device),
                     [int(percent * model.t_steps) - 1 for _ in range(len(imgs))]),
                f"Forward Diffusion: {int(percent * 100)}% noise"
            )
        break

# Display the forward diffusion process
displayForward(model, dataloader, device)

"""#Training the model"""

def genNewImages(model, n_samples=16, device=None, c=3, h=IMAGE_SIZE, w=IMAGE_SIZE):
    with torch.no_grad():
        if device is None:
            device = model.device

        # Starting from random noise
        x = torch.randn(n_samples, c, h, w).to(device)

        for idx, t in enumerate(list(range(model.t_steps))[::-1]):
            # Estimating noise to be removed
            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()
            eta_theta = model.backward(x, time_tensor)

            alpha_t = model.alphas[t]
            alpha_t_bar = model.alpha_bars[t]

            # Partially denoising the image
            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)

            if t > 0:
                z = torch.randn(n_samples, c, h, w).to(device)

                beta_t = model.betas[t]
                sigma_t = beta_t.sqrt()

                x = x + sigma_t * z
    return x

def train(model, loader, n_epochs, optim, device, store_path, display=False):
    mse = CustomMSELoss()
    best_loss = float("inf")
    t_steps = model.t_steps
    show_every = 10   # Change this to change the freq of display images
    model.train()
    epoch_losses = []
    
    for epoch in tqdm(range(n_epochs), desc=f"Training progress", colour="#00ff00"):
        epoch_loss = 0.0
        for step, batch in enumerate(tqdm(loader, leave=False, desc=f"Epoch {epoch + 1}/{n_epochs}", colour="#005500")):

            x0 = batch[0].to(device)
            n = len(x0)

            # Sampling some noise for current batch
            eta = torch.randn_like(x0).to(device)
            t = torch.randint(0, t_steps, (n,)).to(device)

            # Computing the noisy image based on x0 and the time-step (forward process)
            noisy_imgs = model(x0, t, eta)

            # Computing model estimation
            eta_theta = model.backward(noisy_imgs, t.reshape(n, -1))

            # Optimizing the MSE between the noise plugged and the predicted noise
            loss = mse(eta_theta, eta)

            optim.zero_grad()
            loss.backward()
            optim.step()

            epoch_loss += loss.item() * len(x0) / len(loader.dataset)

        epoch_losses.append(epoch_loss)

        # Display images generated at every {show_every} epoch
        if (epoch == 0 or (epoch + 1) % show_every == 0 or epoch + 1 == n_epochs):
          if display:
              displayImagesSub(genNewImages(model, device=device), f"Images generated at epoch {epoch + 1}")
              model.train()

        log_string = f"Loss at epoch {epoch + 1}: {epoch_loss:.3f}"

        # Storing the best model
        if best_loss > epoch_loss:
            best_loss = epoch_loss
            torch.save(model.state_dict(), store_path)
            log_string += " --> Best model yet (stored in drive)"

        print(log_string)
    
    plotLoss(epoch_losses)

# Training hyperparameters
n_epochs = 150
lr = 0.001
optimizer = optim.Adam(model.parameters(), lr=0.001)

model_save_path = '/content/drive/MyDrive/Assignment07/Saved_Models/'
model_save_name = model_save_path + f'epoch{n_epochs}_Tis{t_steps}model_unet_denoise.pth'

train(model, dataloader, n_epochs, optim=optimizer, device=device, store_path=model_save_name, display=True)

"""#Generate New Images"""

# Loading the trained model
best_model = MyDiffusion(MyUNet(t_steps), t_steps=t_steps, min_beta=min_beta, max_beta=max_beta, device=device)
best_model.load_state_dict(torch.load(model_save_name, map_location=device))
best_model.eval()
print("Model loaded")

print("Generating new images")
generated = genNewImages(best_model, n_samples=16, device=device)
displayImagesSub(generated, "Final result")

# End of code